{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningAgents import ReinforceAlgorithm\n",
    "from environmentModel import Model, AdversaryModes\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import multiprocess as mp\n",
    "from multiprocess import Process\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "adversaryProbs[0]=1\n",
    "game = Model(totalDemand = 400, tupleCosts = (57, 71),\n",
    "              totalStages = 25, adversaryProbs=adversaryProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet=NeuralNetwork(lr = 0.001, num_input= 2, num_actions=50, nn_dim = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565.65802693367\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "AllResults = list()\n",
    "if __name__ == '__main__':\n",
    "    PROCESSES = 8\n",
    "    ctx = mp.get_context('spawn')\n",
    "    with ctx.Pool(PROCESSES) as pool:\n",
    "# ReinforceAlgorithm(game, neuralNet, numberIterations, numberEpiPerBatch, numberBatches, discountFactor, creditFactor)\n",
    "        params = [(game, neuralNet, 1, 1000, 10, 1, 0.2,), \n",
    "                  (game, neuralNet, 1, 1000, 10, 1, 0.4,),\n",
    "                  (game, neuralNet, 1, 1000, 10, 1, 0.6,),\n",
    "                  (game, neuralNet, 1, 1000, 10, 1, 0.8,)]\n",
    "        \n",
    "        results = [pool.apply_async(ReinforceAlgorithm, p) for p in params]\n",
    "        for result in results:\n",
    "            result = result.get()\n",
    "            result = result.solver()\n",
    "            AllResults.append(result)\n",
    "et = time.time() \n",
    "print(et - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in AllResults:\n",
    "    if result[3] > 0.99:\n",
    "        print(result[1], result[2], result[3])\n",
    "    else:\n",
    "        print(\"Not yet converged\")\n",
    "    plt.plot(result[0][0])\n",
    "    plt.show()\n",
    "    print(\"Best Payoff is \", result[5])\n",
    "    print(\"Best Actions are \", result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm = ReinforceAlgorithm(game, neuralNet, 1, 10, 5, 1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(algorithm.solver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# learning = pd.DataFrame(algorithm.returns.mean(axis = 0),columns=['entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning = learning.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_2 = [0]*len(learning)\n",
    "# for i in range(len(learning_2)):\n",
    "#     learning_2[i] = learning[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(learning_2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_learning = np.convolve(learning_2, np.ones(1000)/1000, mode = 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(avg_learning)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
