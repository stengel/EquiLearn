{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningAgent import LearningAlgorithm\n",
    "from environment import Model, AdversaryModes\n",
    "from Qtable import QTable\n",
    "from test import Test\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_demand = 400\n",
    "agent_cost = 57\n",
    "adversary_cost = 71 \n",
    "costs = [agent_cost,adversary_cost]\n",
    "total_stages = 25\n",
    "adversary_probabilities=[0]*len(AdversaryModes)\n",
    "# Below is where we decide what adversaries we train against- see environment.py for the numbers\n",
    "# Replace * and ** with the two number associated with the opponents at the bottom of environment.py\n",
    "adversary_probabilities[*]= 0.5\n",
    "adversary_probabilities[**] = 0.5\n",
    "game = Model(total_demand, costs, total_stages, adversary_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_actions = 50\n",
    "number_demands = total_demand + 1\n",
    "discount_factor = 1\n",
    "number_episodes = 100_000_000\n",
    "constant = int(number_episodes/49)\n",
    "print(constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is given as [numerator,denominator] which gives us a learning rate function of \n",
    "# numerator/(n+denominator)\n",
    "Qtable = QTable(number_demands, number_actions, total_stages , learning_rate = [constant,constant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = LearningAlgorithm(game, Qtable, number_episodes, discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the first stage of learning- actions are chosen randomly.\n",
    "number_episodes_per_round = 100_000\n",
    "number_rounds = int(number_episodes / number_episodes_per_round)\n",
    "errors = np.zeros(number_rounds)\n",
    "final_round = 0\n",
    "for round_ in range(number_rounds):\n",
    "    algorithm.continue_learning(number_episodes_per_round,number_episodes_per_round * round_ +1)\n",
    "    result = Test(game, Qtable, discount_factor, adversary_probabilities)\n",
    "    errors[round_] = result.error(1000)\n",
    "    if round_ % 50 == 0:\n",
    "        print(round_, errors[round_])\n",
    "    if round_ > 10 and np.max(errors[round_-10:round_]) < 0.01:\n",
    "        print(round_)\n",
    "        final_round = round_\n",
    "        break\n",
    "plt.plot(errors[0:final_round+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the second stage of learning- actions are chosen according to an epsilon greedy strategy.\n",
    "episode_counter = (final_round + 1) * number_episodes_per_round\n",
    "episodes_left = number_episodes - episode_counter\n",
    "number_episodes_per_round = 500_000\n",
    "number_rounds = int(episodes_left / number_episodes_per_round)\n",
    "for round_ in range(number_rounds): \n",
    "    print('Round ', round_, ' of ', number_rounds)\n",
    "    algorithm.epsilon_greedy_learning(number_episodes_per_round, episode_counter)\n",
    "    result = Test(game, Qtable, discount_factor, adversary_probabilities)\n",
    "    payoff, _, actions, _, _ = result.total_payoff()\n",
    "    print('Current payoff: ', payoff)\n",
    "    print('Current actions:', actions)\n",
    "    episode_counter += number_episodes_per_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have the trained agent. We want to test it against each opponent individually, so that\n",
    "# we can compare it against the agent that is just trained against this opponent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_probabilities=[0]*len(AdversaryModes)\n",
    "# Testing against one type of opponent by changing the * below to be the first number\n",
    "adversary_probabilities[*]=1\n",
    "result = Test(game, Qtable, discount_factor, adversary_probabilities)\n",
    "payoff, adversary_payoff, actions, adversary_actions, demand_potential = result.total_payoff()\n",
    "print(payoff)\n",
    "print(adversary_payoff)\n",
    "print(actions)\n",
    "print(adversary_actions)\n",
    "print(demand_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_probabilities=[0]*len(AdversaryModes)\n",
    "# Testing against one type of opponent by changing the * below to be the first number\n",
    "adversary_probabilities[**]=1\n",
    "result = Test(game, Qtable, discount_factor, adversary_probabilities)\n",
    "payoff, adversary_payoff, actions, adversary_actions, demand_potential = result.total_payoff()\n",
    "print(payoff)\n",
    "print(adversary_payoff)\n",
    "print(actions)\n",
    "print(adversary_actions)\n",
    "print(demand_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
